{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAQLAIN-MUSHTAQUE/HassanZindani-ResumeScreening/blob/main/Resume_ScreeningV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRBhSynKSaJc"
      },
      "source": [
        "# CV Analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCxL38OSdzI"
      },
      "source": [
        "## module install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWE2zxMbaZR4",
        "outputId": "d9b55fea-bbea-407f-ab9f-44fb7784a62a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 0 B/129 kB 0%] [Waiting for headers] [Connected t\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,163 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,391 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,672 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,319 kB]\n",
            "Fetched 11.4 MB in 4s (2,582 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (414 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update #Update the library of Linux\n",
        "!apt-get install -y poppler-utils #Store temporary data of another library (PDF2image)\n",
        "# !apt-get install -y libreoffice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yr63DwLpSZ62",
        "outputId": "b9909fc5-5c59-4728-bc05-16a7c3dbba57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.3/389.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain_community docx2txt pinecone-client tiktoken langchain langchain-openai pypdf rapidocr-onnxruntime pdf2image loguru openai tiktoken langchain asyncio aiohttp  -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zssQVzqzrTaY"
      },
      "outputs": [],
      "source": [
        "import os #Basic module\n",
        "import requests\n",
        "import asyncio #Making a parallel programming\n",
        "import aiohttp\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pdf2image import convert_from_path #PDF2image conversion\n",
        "from PIL import Image #PDF2image conversion\n",
        "import base64 #image format into binary format\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "import io\n",
        "import tempfile\n",
        "from google.colab import userdata\n",
        "from openai import AsyncOpenAI\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from loguru import logger\n",
        "import tiktoken\n",
        "from google.colab import files\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import concurrent.futures\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "from pinecone import  Pinecone, ServerlessSpec\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_community.document_loaders.dataframe import DataFrameLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from itertools import islice\n",
        "from typing import Iterable, Iterator, List, Any, TypeVar, Dict, Tuple, Union, Optional\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Credential"
      ],
      "metadata": {
        "id": "Jz7QtFckGnGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI Client\n",
        "client = AsyncOpenAI(api_key = userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "JRrmotzCGqcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loaders**"
      ],
      "metadata": {
        "id": "9QGMI_EfO1ZS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItximtAWq97E"
      },
      "source": [
        "### PDF Reader Using OpenAI Vision (multiple file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lDuNPnJrl0J"
      },
      "source": [
        "#### PDF to Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLqlTvBLq9sm"
      },
      "outputs": [],
      "source": [
        "def pdf_page_to_image(pdf_path):\n",
        "    \"\"\"Convert PDF pages to images.\"\"\"\n",
        "    try:\n",
        "      pdf = pdf_path.name\n",
        "    except:\n",
        "      pdf = pdf_path\n",
        "\n",
        "    pages = convert_from_path(pdf, 300)  # 300 DPI for high quality\n",
        "    return pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xodx1CAVrttG"
      },
      "source": [
        "#### OpenAI Vision Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK-Dbkz6rvqX"
      },
      "outputs": [],
      "source": [
        "async def encode_image(image_path):\n",
        "    \"\"\"Encode image as a base64 string.\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "async def count_tokens_async(text, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"Count the number of tokens in a text using tiktoken.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "async def send_image_to_openai_async(image_path, api_key, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"Send an image to OpenAI API and receive the extracted text.\"\"\"\n",
        "    base64_image = await encode_image(image_path)\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    prompt = \"\"\"Extract the exact text from the provided image without adding any additional lines at the beginning.\n",
        "    \"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 1200\n",
        "    }\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
        "            response_data = await response.json()\n",
        "            output_text = response_data['choices'][0]['message']['content']\n",
        "\n",
        "    # Calculate tokens used\n",
        "    input_tokens = await count_tokens_async(prompt, model=model)\n",
        "    output_tokens = await count_tokens_async(output_text, model=model)\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "\n",
        "    # Pricing details\n",
        "    cost_per_input_token = 0.150 / 1_000_000  # $ per input token\n",
        "    cost_per_output_token = 0.600 / 1_000_000  # $ per output token\n",
        "    cost_per_image = 0.01275  # $ per image (300x300 px)\n",
        "\n",
        "    # Calculate total cost\n",
        "    input_cost = input_tokens * cost_per_input_token\n",
        "    output_cost = output_tokens * cost_per_output_token\n",
        "    total_cost = cost_per_image + input_cost + output_cost\n",
        "\n",
        "    return {\n",
        "        \"content\": output_text,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"total_cost\": total_cost\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text to Langchain Document"
      ],
      "metadata": {
        "id": "T-KQrsDPzR8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_texts_with_sources(text_source_pairs):\n",
        "    \"\"\"\n",
        "    Converts a list of text-source pairs into LangChain document format.\n",
        "\n",
        "    Args:\n",
        "        text_source_pairs (list of tuples): Each tuple contains (text, source).\n",
        "\n",
        "    Returns:\n",
        "        list of Document: LangChain documents with content and metadata.\n",
        "    \"\"\"\n",
        "    documents = [\n",
        "        Document(page_content=text, metadata={\"source\": source})\n",
        "        for text, source in text_source_pairs\n",
        "    ]\n",
        "    return documents"
      ],
      "metadata": {
        "id": "UOrTLypBzPYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwPNyfVdr2G-"
      },
      "source": [
        "#### Process image to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2FgaDOyr6wN"
      },
      "outputs": [],
      "source": [
        "async def process_image_text(images, api_key, vision_model=\"gpt-4o-mini\", filename=\"document.pdf\"):\n",
        "    \"\"\"Process images and extract text as DocumentPage objects with metadata.\"\"\"\n",
        "    total_tokens = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    async def process_single_image(image, image_index):\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as temp_file:\n",
        "            image.save(temp_file, format=\"PNG\")\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        # Process the image and extract text\n",
        "        result = await send_image_to_openai_async(temp_file_path, api_key, vision_model)\n",
        "        os.remove(temp_file_path)  # Remove the temporary file after processing\n",
        "\n",
        "        # return text, total token and the total cost\n",
        "        return result[\"content\"], float(result[\"total_tokens\"]), float(result[\"total_cost\"])\n",
        "\n",
        "    # Create tasks for each image\n",
        "    tasks = [process_single_image(image, i) for i, image in enumerate(images, 1)]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Collect results\n",
        "    all_text = \"\"\n",
        "    for doc, tokens, cost in results:\n",
        "        all_text += doc + \"\\n\"\n",
        "        total_tokens += tokens\n",
        "        total_cost += cost\n",
        "\n",
        "\n",
        "    # Converting into text pair\n",
        "    text_source_pairs = [(all_text, filename)]\n",
        "\n",
        "    # Converting into Langchain document\n",
        "    documents = load_texts_with_sources(text_source_pairs)\n",
        "\n",
        "    return documents, total_tokens, total_cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MS word loader"
      ],
      "metadata": {
        "id": "BUN9ssbjf44V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docx(file_path):\n",
        "    loader = Docx2txtLoader(file_path)\n",
        "    loader =  loader.load()\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "eNJvNVvNgBR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Splitter"
      ],
      "metadata": {
        "id": "KwlrkaAJ9BGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_seperator(docs):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\\n\\n\", \"\\n\\n\\n\", \"\\n\\n\", \"\\n• \", \"\\n○ \", \"\\n- \", \"\\t\", \"\\n\"],\n",
        "        chunk_size=100,\n",
        "        chunk_overlap = 15,\n",
        "    )\n",
        "\n",
        "    splitted_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "    return splitted_docs"
      ],
      "metadata": {
        "id": "w-CskXlkgm38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process single file"
      ],
      "metadata": {
        "id": "V_T-PmvEyMjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import meta_path\n",
        "# Processing Function single file\n",
        "async def process_single_file(indx: int, filename: str, api_key: str, vision_model: str):\n",
        "    \"\"\"Asynchronously process a single file by converting to images and extracting text.\"\"\"\n",
        "    src_path = os.path.join(\"/content\", filename)\n",
        "    dest_path = os.path.join(\"/content/CV_folder\", filename)\n",
        "    os.rename(src_path, dest_path)\n",
        "\n",
        "    # Get the file path\n",
        "    file_path = dest_path\n",
        "    _, ext = os.path.splitext(file_path)\n",
        "    ext = ext.lower()\n",
        "\n",
        "    total_tokens = 0.0\n",
        "    total_cost = 0.0\n",
        "\n",
        "    if ext == \".docx\":\n",
        "        langchain_documents = load_docx(file_path)  # Assuming load_docx is implemented similarly with metadata\n",
        "    elif ext == \".pdf\":\n",
        "        # Convert PDF pages to images\n",
        "        images = pdf_page_to_image(file_path)\n",
        "        print(f\"Conversion for file {filename} has been done!\")\n",
        "\n",
        "        # Extract text from images asynchronously with Langchain-compatible output\n",
        "        langchain_documents, total_tokens, total_cost = await process_image_text(images, api_key, vision_model=vision_model, filename=dest_path)\n",
        "        print(f\"Text extraction for file {filename} is complete!\")\n",
        "\n",
        "    # Raw Text\n",
        "    all_text= \"\"\n",
        "    for doc in langchain_documents:\n",
        "      all_text += doc.page_content + \"\\n\"\n",
        "\n",
        "    # Splitting document\n",
        "    splitted_docs = text_seperator(langchain_documents)\n",
        "\n",
        "    # Key (for each file) to store extracted data\n",
        "    key = dest_path\n",
        "\n",
        "    result = {\n",
        "        \"splitted_docs\": splitted_docs,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"total_cost\": total_cost\n",
        "    }\n",
        "\n",
        "    return key, result, all_text"
      ],
      "metadata": {
        "id": "v6de-taLyQL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main orchestration Process"
      ],
      "metadata": {
        "id": "bIgSF4GFboge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIR5HszXsBI6"
      },
      "outputs": [],
      "source": [
        "# Main orcheshtration process (engine first part)\n",
        "async def main_process(api_key: str, vision_model: str = \"gpt-4o-mini\"):\n",
        "    \"\"\"Main function to orchestrate the entire process asynchronously.\"\"\"\n",
        "\n",
        "    # Upload the file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Create the CV_folder if it doesn't exist\n",
        "    os.makedirs('/content/CV_folder', exist_ok=True)\n",
        "\n",
        "    # Initiate variables\n",
        "    loaded_docs = {}\n",
        "    all_raw_text = {}\n",
        "    total_tokens = 0.0\n",
        "    total_cost = 0.0\n",
        "\n",
        "    # List to hold tasks for processing each file concurrently\n",
        "    tasks = []\n",
        "\n",
        "    # Move the uploaded files to /content/CV_folder and process them concurrently\n",
        "    for indx, filename in enumerate(uploaded.keys()):\n",
        "        task = process_single_file(indx, filename, api_key, vision_model)\n",
        "        tasks.append(task)\n",
        "\n",
        "    # Run all the tasks concurrently and gather the results\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    # print(results)\n",
        "\n",
        "    # Process the results\n",
        "    for key, result, raw_text in results:\n",
        "      loaded_docs[key] = result['splitted_docs']\n",
        "      all_raw_text[key] = raw_text\n",
        "      total_tokens += result['total_tokens']\n",
        "      total_cost += result['total_cost']\n",
        "\n",
        "\n",
        "\n",
        "    return loaded_docs, all_raw_text, total_tokens, total_cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoqkWpg8sE1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "0eff3cf5-004a-466b-d7da-8927a15c5057"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a54dc13-6a79-41e6-8a3a-cd019453640d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a54dc13-6a79-41e6-8a3a-cd019453640d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Affan Sami resume.pdf to Affan Sami resume.pdf\n",
            "Saving H.M. Saqlain Mushtaque Resume 2023.pdf to H.M. Saqlain Mushtaque Resume 2023.pdf\n",
            "Saving Murtaza_Resume.pdf to Murtaza_Resume.pdf\n",
            "Saving Sharjeel CV.pdf to Sharjeel CV.pdf\n",
            "Saving SM Ghuffran 2022.pdf to SM Ghuffran 2022.pdf\n",
            "Conversion for file Affan Sami resume.pdf has been done!\n",
            "Conversion for file H.M. Saqlain Mushtaque Resume 2023.pdf has been done!\n",
            "Conversion for file Murtaza_Resume.pdf has been done!\n",
            "Conversion for file Sharjeel CV.pdf has been done!\n",
            "Conversion for file SM Ghuffran 2022.pdf has been done!\n",
            "Text extraction for file Murtaza_Resume.pdf is complete!\n",
            "Text extraction for file Sharjeel CV.pdf is complete!\n",
            "Text extraction for file SM Ghuffran 2022.pdf is complete!\n",
            "Text extraction for file Affan Sami resume.pdf is complete!\n",
            "Text extraction for file H.M. Saqlain Mushtaque Resume 2023.pdf is complete!\n"
          ]
        }
      ],
      "source": [
        "loaded_docs,all_raw_text, total_tokens, total_cost = await main_process(userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pinecone Upload**\n",
        "\n"
      ],
      "metadata": {
        "id": "di4mRj3CUwbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinecone Credentials"
      ],
      "metadata": {
        "id": "Lz5aXI-vZdh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# OpenAI Embedding model\n",
        "embedding_model = OpenAIEmbeddings(model = 'text-embedding-3-large')\n",
        "\n",
        "# Defining Index\n",
        "pc = Pinecone(PINECONE_API_KEY)\n"
      ],
      "metadata": {
        "id": "W4-h3mAZU_Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Index\n"
      ],
      "metadata": {
        "id": "YmqnT5trCzO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(host=userdata.get('PINECONE_INDEX_URL'))"
      ],
      "metadata": {
        "id": "WEJTKJt6C4bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinecone Upload Function"
      ],
      "metadata": {
        "id": "BiIYRrefU_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = TypeVar(\"T\")\n",
        "\n",
        "def batch_iterate(size: int, iterable: Iterable[T]) -> Iterator[List[T]]:\n",
        "    \"\"\"Utility batching function.\"\"\"\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        chunk = list(islice(it, size))\n",
        "        if not chunk:\n",
        "            return\n",
        "        yield chunk\n",
        "def upload_data(loaded_docs, index, embedding_model, namespace):\n",
        "    embedding_chunk_size = 150\n",
        "    batch_size = 70 # size of batch (length of chunks in one go.)\n",
        "\n",
        "    ids_value = 0\n",
        "    metadata_list = []\n",
        "    texts = []\n",
        "    ids = []\n",
        "\n",
        "    for key, value in loaded_docs.items():\n",
        "      texts.extend([page.page_content for page in value])\n",
        "      metadata_list.extend([page.metadata for page in value])\n",
        "      ids.extend([str(ids_value + i) for i in range(len(texts))])\n",
        "      ids_value += len(texts)\n",
        "\n",
        "    for metadata, text in zip(metadata_list, texts):\n",
        "      metadata[\"text\"] = text\n",
        "\n",
        "    for i in range(0, len(texts), embedding_chunk_size):\n",
        "      chunk_texts = texts[i : i + embedding_chunk_size]\n",
        "      chunk_ids = ids[i : i + embedding_chunk_size]\n",
        "      chunk_metadatas = metadata_list[i : i + embedding_chunk_size]\n",
        "      embeddings = embedding_model.embed_documents(chunk_texts)\n",
        "\n",
        "      # uploading asynchronously\n",
        "      async_res = [\n",
        "          index.upsert(\n",
        "              vectors=batch,\n",
        "              async_req=True,\n",
        "              namespace=namespace\n",
        "          )\n",
        "          for batch in batch_iterate(\n",
        "              batch_size, zip(chunk_ids, embeddings, chunk_metadatas)\n",
        "          )\n",
        "      ]\n",
        "      [res.get() for res in async_res]"
      ],
      "metadata": {
        "id": "rQTZONTOZg29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Unique Namespace"
      ],
      "metadata": {
        "id": "YInMS4hCtlcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unique_namespace(base_name=\"CVs_data\"):\n",
        "    # Generate a unique index name by appending a UUID\n",
        "    unique_id = uuid.uuid4().hex[:8]  # Get a short unique identifier\n",
        "    return f\"{base_name}_{unique_id}\""
      ],
      "metadata": {
        "id": "nqW25R17tdv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define namespace\n",
        "namespace = generate_unique_namespace()     #  In FastAPI it will be MongoDB Batch (Model) ID"
      ],
      "metadata": {
        "id": "h79Ng9LetwH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "namespace = 'CVs_data_db716073'"
      ],
      "metadata": {
        "id": "kdaJxa1lllxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading to Pinecone"
      ],
      "metadata": {
        "id": "VlhdygU9Tqtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upload_data(loaded_docs, index, embedding_model, namespace)"
      ],
      "metadata": {
        "id": "sRClRh-ZKu15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3c7ec927-a67a-401c-d7b5-141db134b815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loaded_docs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-956f2a963671>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loaded_docs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking length of any namespace"
      ],
      "metadata": {
        "id": "whAkjugvCUWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats = index.describe_index_stats()\n",
        "\n",
        "# Check the namespaces and their vector counts\n",
        "namespace_stats = stats['namespaces']\n",
        "count = namespace_stats.get(namespace,{}).get('vector_count', 0)\n",
        "print(f\"Number of entries in the namespace '{namespace}': {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2wEB_DUCaJb",
        "outputId": "e1d0797e-e3a1-4c5a-cd4a-146a5eb0a99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries in the namespace 'CVs_data_db716073': 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query:"
      ],
      "metadata": {
        "id": "l3hQGUayuFtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Job Post"
      ],
      "metadata": {
        "id": "NTe3G_vTKQdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "As a DevOps Engineers at Dubizzle ensures, you will ensure our services are healthy, monitored, automated, and designed to scale. You'll use your background as an operations generalist to work closely with our development teams from the early stages of design all the way through identifying and resolving production issues. You will support a wide range of products focusing on automation, availability and performance, and above all reliability.\n",
        "\n",
        "\n",
        "Requirements\n",
        "\n",
        "Serve as a primary point responsible for the overall health, performance, efficiency and capacity of our production environment\n",
        "Be part of the implementation and design of the systems used to operate Dubizzle, with a focus on automation and maintainability at large scale\n",
        "Experience in building large CI/CD processes (experience with Github actions & Jenkins is preferred)\n",
        "Collaboration with the development team on operations-related issues, providing support and acting as stakeholder\n",
        "Provide fast and reliable development experience for product teams by maintaining production-like environments used for development and testing\n",
        "Develop tools to effectively monitor custom applications in a large-scale environment\n",
        "Troubleshoot issues across the entire stack - hardware, software, application and network\n",
        "Migrate applications off of legacy environments with minimal downtime\n",
        "Take part in a shared 24x7 on-call rotation\n",
        "Document system design and procedures\n",
        "Experience & Qualification\n",
        "\n",
        "Bachelor's Degree in Computer Science, Engineering or equivalent field and significant experience in DevOps\n",
        "AWS Solutions Architect certification\n",
        "4+ years experience in supporting hosted services in a high-volume customer-facing environment\n",
        "Experience with DevOps tools, processes, culture, distributed systems, supporting mission-critical systems\n",
        "Experience working on Continuous Integration / Delivery pipelines using tools such as Jenkins and Github actions\n",
        "Demonstrated ability to write programs / utilities using a high-level programming language like: Go, Ruby, Python as well as Bash\n",
        "3+ years proven experience in running and managing infrastructure at scale using container orchestration tool kubernetes\n",
        "Experience in performing migration from traditional technology stack(s) to AWS EKS or similar\n",
        "\n",
        "\n",
        "Benefits\n",
        "\n",
        "\n",
        "\n",
        "fast paced, high performing team\n",
        "ulticultural environment with over 40 different nationalities\n",
        "omprehensive Health insurance (Spouse & Children)\n",
        "ewards & Recognitions\n",
        "earning & Development opportunities\n",
        "ym Allowance\n",
        "aptop Ownership\n",
        "aid Leaves\n",
        "\n",
        "Benefits found in job post\n",
        "\n",
        "Medical insurance\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SC5iXkDkuJjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quary System Prompt"
      ],
      "metadata": {
        "id": "2w7nZWQwbCC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_prompt = \"\"\"\n",
        "You are an excellent Job Post Analyzer and vector database query writer. You will provided a job post and your job is to analyze it and extract the following information that can be used to search the vector database.\n",
        "\n",
        "**IMPORTANT:** Write the information in your response like that can be used to search the vector database.\n",
        "\n",
        "Information_fields:\n",
        "job_title: The title of the job. It must be present in the job posting. It should be string \"\". This information is compulsory and not empty, and if there is no job title, then this is not a valid job post, and in that case, simply return \"Invalid Job Post\" without searching for the further field's information.\n",
        "job_description: The description of the job. It must be present in the job posting. It should be string \"\". If there is no job description, then return None.\n",
        "required_experience: The experience or the level of the candidate required for the job. This information may describe the level of experience or the duration of experience that candidate must have. If there is more than one required experience, then write the job role with the required number of durations of experience. This information should be the string \"\". This information must be present in the job posting. If this information is not present, then write your remarks about the experience according to the Job.\n",
        "job_reponsibilities_required: The list of responsibilities or the roles for the given job that candidate must have. This information should be written in comma separated value inside the string \"\". This information must be present in the job post. If this information is not present, then write your remarks about the responsibilities according to the Job post.\n",
        "job_reponsibilities_optional: The list of responsibilities or the roles for the given job that candidate can have. This information should be written in comma separated value inside the string \"\". This information may be present in the job posting. If there is no optional responsibilities, then return None.\n",
        "required_education: The list of education required for the job. This information should be written in comma separated value inside the string \"\". This information must be present in the job posting. If this information is not present, then write your remarks about the education according to the Job post.\n",
        "required_degree: The degree or list of qualifications required for the job. This information should be written in comma separated value inside the string \"\". If there is no degree required, then return None.\n",
        "required_skills: The list of skills required for the job. This information should be written in comma separated value inside the string \"\". This information must be present in the job posting. If this information is not present, then write your remarks about the skill for the given Job post.\n",
        "required_location: The requirement of location for the job. This information should be written in comma separated value inside the string \"\". This information is not compulsory. If there is no location required, then return None.\n",
        "extra_info: All the other information that are not listed in the above fields. This information should be in detail without leaving any single information that are not addressed above. This information should be written in comma separated value inside the string \"\". This information may be present in the job posting. If there is no extra information, then return None.\n",
        "\n",
        "Your output should be in the JSON format:\n",
        "{{\n",
        "  \"job_title\": \"\",\n",
        "  \"job_description\": \"\" || None,\n",
        "  \"required_experience\": \"\",\n",
        "  \"job_reponsibilities_required\": \"\",\n",
        "  \"job_reponsibilities_optional\": \"\" || None,\n",
        "  \"required_education\": \"\",\n",
        "  \"required_degree\": \"\" || None,\n",
        "  \"required_skills\": \"\" ,\n",
        "  \"required_location\": \"\" || None,\n",
        "  \"extra_info\": \"\" || None\n",
        "}}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cOk5hfRWbA3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass to the LLM (OpenAI)"
      ],
      "metadata": {
        "id": "T6KGb3yWbIF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def extract_query_info(client, query_system_prompt, query, llm_model = \"gpt-4o-2024-08-06\"):\n",
        "\n",
        "\n",
        "   # User message\n",
        "    user_message = f\"\"\"Extract the information from the job post text: {query}\"\"\"\n",
        "\n",
        "    # Generate the completion\n",
        "    completion = await client.chat.completions.create(\n",
        "        model=llm_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": query_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "    )\n",
        "\n",
        "    # Get the response message\n",
        "    json_message = completion.choices[0].message.content\n",
        "    message = json.loads(json_message)\n",
        "\n",
        "    return message"
      ],
      "metadata": {
        "id": "uoOe3d9gbLtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_query_info= await  extract_query_info(client, query_system_prompt = query_prompt, query= query, llm_model = \"gpt-4o-2024-08-06\")"
      ],
      "metadata": {
        "id": "SWgXEZ9obR0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_query_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2hqpJW3bfjT",
        "outputId": "d9a5a8b3-3096-4bac-d918-af45703cfc5d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_title': 'DevOps Engineer',\n",
              " 'job_description': \"As a DevOps Engineers at Dubizzle ensures, you will ensure our services are healthy, monitored, automated, and designed to scale. You'll use your background as an operations generalist to work closely with our development teams from the early stages of design all the way through identifying and resolving production issues. You will support a wide range of products focusing on automation, availability and performance, and above all reliability.\",\n",
              " 'required_experience': '4+ years experience in supporting hosted services in a high-volume customer-facing environment, 3+ years proven experience in running and managing infrastructure at scale using container orchestration tool kubernetes',\n",
              " 'job_reponsibilities_required': 'Serve as a primary point responsible for the overall health, performance, efficiency and capacity of our production environment, Be part of the implementation and design of the systems used to operate Dubizzle, Collaboration with the development team on operations-related issues, Provide fast and reliable development experience for product teams, Develop tools to effectively monitor custom applications in a large-scale environment, Troubleshoot issues across the entire stack - hardware, software, application and network, Migrate applications off of legacy environments with minimal downtime, Take part in a shared 24x7 on-call rotation, Document system design and procedures',\n",
              " 'job_reponsibilities_optional': None,\n",
              " 'required_education': \"Bachelor's Degree in Computer Science, Engineering or equivalent field\",\n",
              " 'required_degree': \"Bachelor's Degree, AWS Solutions Architect certification\",\n",
              " 'required_skills': 'Experience in building large CI/CD processes, experience with Github actions & Jenkins, Experience with DevOps tools, processes, culture, distributed systems, supporting mission-critical systems, Experience working on Continuous Integration / Delivery pipelines using tools such as Jenkins and Github actions, Demonstrated ability to write programs / utilities using a high-level programming language like: Go, Ruby, Python as well as Bash, Experience in performing migration from traditional technology stack(s) to AWS EKS or similar',\n",
              " 'required_location': None,\n",
              " 'extra_info': 'fast paced, high performing team, multicultural environment with over 40 different nationalities, comprehensive Health insurance (Spouse & Children), rewards & Recognitions, learning & Development opportunities, gym Allowance, laptop Ownership, paid Leaves, Benefits found in job post, Medical insurance'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filling the query info in the variables"
      ],
      "metadata": {
        "id": "KIYld6thbqIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_data = {\n",
        "    'job_title': extracted_query_info.get('job_title'),\n",
        "    'job_description': extracted_query_info.get('job_description', ''),\n",
        "    'required_experience': extracted_query_info.get('required_experience'),\n",
        "    'job_responsibilities_required': extracted_query_info.get('job_reponsibilities_required'),\n",
        "    'job_responsibilities_optional': extracted_query_info.get('job_reponsibilities_optional', ''),\n",
        "    'required_education': extracted_query_info.get('required_education'),\n",
        "    'required_degree': extracted_query_info.get('required_degree', ''),\n",
        "    'required_skills': extracted_query_info.get('required_skills'),\n",
        "    'required_location': extracted_query_info.get('required_location', ''),\n",
        "    'extra_info': extracted_query_info.get('extra_info', '')\n",
        "}\n"
      ],
      "metadata": {
        "id": "ykRLRUfWbtsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils: Supporting Function\n"
      ],
      "metadata": {
        "id": "CMd6tC0nqK1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supporting Functions\n",
        "def transform_and_sort_response(response):\n",
        "    # Transform data to include text and sum of scores\n",
        "    transformed_data = {\n",
        "        file_path: {\n",
        "            'text': details['text'],\n",
        "            'score': max(details['score'])\n",
        "        }\n",
        "        for file_path, details in response.items()\n",
        "    }\n",
        "\n",
        "    # Sort the transformed data by score_sum in descending order\n",
        "    sorted_data = dict(sorted(transformed_data.items(), key=lambda item: item[1]['score'], reverse=True))\n",
        "    return sorted_data\n",
        "\n",
        "def tok_k_function(index,namespace):\n",
        "    # Calculating top-k using index\n",
        "    stats = index.describe_index_stats()\n",
        "    # check the namespaces and their vector counts\n",
        "    namespace_stats = stats['namespaces']\n",
        "    namespace= namespace\n",
        "    experience_top_k= namespace_stats.get(namespace,{}).get('vector_count', 0)\n",
        "\n",
        "    return experience_top_k\n",
        "\n",
        "# Helper function to handle querying and extracting matching\n",
        "async def query_matching(query: str, namespace: str, top_k: int):\n",
        "    \"\"\"\n",
        "    Queries the Pinecone service with the provided query string, namespace, and top_k value.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to search for.\n",
        "        namespace (str): The namespace in Pinecone to query against.\n",
        "        top_k (int): The number of top results to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of fund dictionaries returned from Pinecone.\n",
        "    \"\"\"\n",
        "    return await query_pinecone(query=query, namespace=namespace, top_k=top_k)\n",
        "\n",
        "\n",
        "# Define top_k\n",
        "top_k = tok_k_function(index,namespace)\n",
        "\n",
        "async def filtering_other_field(job_query, filter_list, max_score, top_k:int=top_k):\n",
        "    tasks = [\n",
        "        query_pinecone(query=job_query, namespace=namespace, top_k=top_k, filter={\"source\": {\"$eq\": name}})\n",
        "        for name in filter_list\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    for completed_task in asyncio.as_completed(tasks):\n",
        "        result = await completed_task\n",
        "        results.append(result)  # Collect results as they complete\n",
        "\n",
        "    flattened_results = [item for sublist in results for item in sublist]  # Flatten the list of lists\n",
        "\n",
        "    final_results = {}\n",
        "\n",
        "    # Filtering score\n",
        "    for item in flattened_results:\n",
        "        if item['score'] > max_score:\n",
        "            file_name = item['metadata']['source']\n",
        "            if file_name not in final_results:\n",
        "                final_results[file_name] = {}\n",
        "                final_results[file_name]['score'] = [item['score']]\n",
        "                final_results[file_name]['reason'] = f\"The field matched with the field of the candidate\"\n",
        "                final_results[file_name]['text'] = item['metadata']['text']\n",
        "            else:\n",
        "                final_results[file_name]['score'].append(item['score'])\n",
        "                final_results[file_name]['text'] += \" \" + item['metadata']['text']\n",
        "\n",
        "    # Transform recent dictionary\n",
        "    transformed_dict = transform_and_sort_response(final_results)\n",
        "\n",
        "    return transformed_dict\n",
        "\n",
        "# Merging Dictionary Function\n",
        "def merge_dictionaries(scoring_dict, *other_dicts):\n",
        "    scoring_dict_copy = scoring_dict.copy()  # Create a copy to avoid modifying the original\n",
        "    for data_dict in other_dicts:\n",
        "        for filename, data in data_dict.items():\n",
        "            if filename in scoring_dict_copy:\n",
        "                # Append the new text to the existing text\n",
        "                scoring_dict_copy[filename]['text'] += ' ' + data['text']\n",
        "                # Add the new score to the existing score\n",
        "                scoring_dict_copy[filename]['score'] += data['score']\n",
        "            else:\n",
        "                # Create a new entry if the filename does not exist in scoring_dict_copy\n",
        "                scoring_dict_copy[filename]['text'] = data['text']\n",
        "                scoring_dict_copy[filename]['score'] = data['score']\n",
        "\n",
        "    for filename, value in scoring_dict_copy.items():\n",
        "        print(f\"Before increment: {value['score']}\")\n",
        "        scoring_dict_copy[filename]['score'] = (value['score']/0.5)\n",
        "        print(f\"After increment: {value['score']}\")\n",
        "\n",
        "    return scoring_dict_copy\n",
        "\n",
        "\n",
        "# Function to Add CV Text into merged_dict\n",
        "def adding_cv_text(merged_scoring_dict, all_raw_text):\n",
        "  for filename, data in all_raw_text.items():\n",
        "    if filename in merged_scoring_dict:\n",
        "      merged_scoring_dict[filename]['CV_text'] = data\n",
        "  return merged_scoring_dict"
      ],
      "metadata": {
        "id": "YIFMJYaYn3Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Function"
      ],
      "metadata": {
        "id": "0bT8d3eZR_wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define top_k\n",
        "top_k = tok_k_function(index,namespace)\n",
        "async def query_pinecone(query: str, namespace: str = namespace, index = index, top_k: int= top_k , filter: dict = None) -> list:\n",
        "    \"\"\"\n",
        "    Query the Pinecone index with the given parameters.\n",
        "\n",
        "    Args:\n",
        "        query: The query string to be embedded and searched.\n",
        "        namespace: The namespace to query within.\n",
        "        top_k: The number of top results to return.\n",
        "        filter: Optional dictionary for metadata filtering.\n",
        "\n",
        "    Returns:\n",
        "        list:The query responses from Pinecone.\n",
        "    \"\"\"\n",
        "    # Embed the query string\n",
        "    loop = asyncio.get_running_loop()\n",
        "    with ThreadPoolExecutor() as pool:\n",
        "        # Embedding in thread pool\n",
        "        embedded = await loop.run_in_executor(pool, embedding_model.embed_query, query)\n",
        "\n",
        "        # Pinecone query in thread pool\n",
        "        query_params = {\n",
        "            \"namespace\": namespace,\n",
        "            \"vector\": embedded,\n",
        "            \"top_k\": top_k,\n",
        "            \"include_metadata\": True,\n",
        "        }\n",
        "        if filter:\n",
        "            query_params[\"filter\"] = filter\n",
        "\n",
        "        try:\n",
        "            responses = await loop.run_in_executor(pool, lambda: index.query(**query_params))\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Pinecone query: {e}\")\n",
        "            return []\n",
        "\n",
        "    # Extract essential data\n",
        "    results = []\n",
        "    for match in responses[\"matches\"]:\n",
        "        # print (f\"{query} \\n {match['id']}\")\n",
        "        results.append({\n",
        "            \"id\": match[\"id\"],\n",
        "            \"score\": match[\"score\"],\n",
        "            \"metadata\": match[\"metadata\"],\n",
        "            \"values\":match[\"values\"]\n",
        "        })\n",
        "    return results  # Return a list of dictionaries instead of Pinecone matches\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KP2aYbhUPCBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering"
      ],
      "metadata": {
        "id": "bX2nXuxEavdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def filtering_skill_edu_exp(job_data, top_k = 1):\n",
        "\n",
        "  # Create asynchronous tasks for fund queries\n",
        "  if 'required_experience' in job_data and job_data['required_experience'] != \"\":\n",
        "    query = job_data['required_experience']\n",
        "    task1 = [query_pinecone(query = query)]\n",
        "\n",
        "      # 'skill_matching': query_matching(diversity_focused, \"funds-diversity-focus\", top_k_diversity_focus),\n",
        "  if 'required_education' in job_data and job_data['required_education'] != \"\":\n",
        "    query = job_data['required_education']\n",
        "\n",
        "    task2 = [query_pinecone(query = query)]\n",
        "\n",
        "  if 'required_skills' in job_data and job_data['required_skills'] != \"\":\n",
        "    query = job_data['required_skills']\n",
        "    task3 = [query_pinecone(query = query)]\n",
        "\n",
        "  if 'job_responsibilities_required' in job_data and job_data['job_responsibilities_required'] != \"\":\n",
        "    query = job_data['job_responsibilities_required']\n",
        "    task4 = [query_pinecone(query = query)]\n",
        "\n",
        "  # Run task1 and task2 concurrently\n",
        "  raw_results1, raw_results2, raw_results3, raw_results4 = await asyncio.gather(\n",
        "      asyncio.gather(*task1),  # Gather all results from task1\n",
        "      asyncio.gather(*task2),  # Gather all results from task2\n",
        "      asyncio.gather(*task3),   # Gather all results from task3\n",
        "      asyncio.gather(*task4)   # Gather all results from task4\n",
        "  )\n",
        "  results1 = [item for sublist in raw_results1 for item in sublist]  # Flatten the list of lists\n",
        "  results2 = [item for sublist in raw_results2 for item in sublist]  # Flatten the list of lists\n",
        "  results3 = [item for sublist in raw_results3 for item in sublist]  # Flatten the list of lists\n",
        "  results4 = [item for sublist in raw_results4 for item in sublist]  # Flatten the list of lists\n",
        "\n",
        "  final_results = {}\n",
        "\n",
        "  for r1 in results1:\n",
        "      file_name = r1['metadata']['source']\n",
        "      if r1['score'] >=0.5:\n",
        "          if file_name not in final_results:\n",
        "              final_results[file_name] = {}\n",
        "              final_results[file_name]['score'] = [r1['score']]\n",
        "              final_results[file_name]['reason'] = f\"The job experience field matched with the experience of the candidate\"\n",
        "              final_results[file_name]['text'] = r1['metadata']['text']\n",
        "          else:\n",
        "              final_results[file_name]['score'].append(r1['score'])\n",
        "              final_results[file_name]['text'] += \" \" + r1['metadata']['text']\n",
        "\n",
        "  for r2 in results2:\n",
        "      file_name = r1['metadata']['source']\n",
        "      if r2['score'] >=0.5:\n",
        "          if file_name not in final_results:\n",
        "              final_results[file_name] = {}\n",
        "              final_results[file_name]['score'] = [r2['score']]\n",
        "              final_results[file_name]['reason'] = f\"The education matched with the education of the candidate\"\n",
        "              final_results[file_name]['text'] = r2['metadata']['text']\n",
        "          else:\n",
        "              final_results[file_name]['score'].append(r2['score'])\n",
        "              final_results[file_name]['text'] += \" \" + r2['metadata']['text']\n",
        "\n",
        "  for r3 in results3:\n",
        "      file_name = r3['metadata']['source']\n",
        "      if r3['score'] >=0.5:\n",
        "          if file_name not in final_results:\n",
        "              final_results[file_name] = {}\n",
        "              final_results[file_name]['score'] = [r3['score']]\n",
        "              final_results[file_name]['reason'] = f\"The skills matched with the skills of the candidate\"\n",
        "              final_results[file_name]['text'] = r3['metadata']['text']\n",
        "          else:\n",
        "              final_results[file_name]['score'].append(r3['score'])\n",
        "              final_results[file_name]['text'] += \" \" + r3['metadata']['text']\n",
        "\n",
        "  for r4 in results4:\n",
        "    file_name = r4['metadata']['source']\n",
        "    if r4['score'] >=0.5:\n",
        "        if file_name not in final_results:\n",
        "            final_results[file_name] = {}\n",
        "            final_results[file_name]['score'] =[r4['score']]\n",
        "            final_results[file_name]['reason'] = f\"The responsibilities matched with the responsibilities of the candidate\"\n",
        "            final_results[file_name]['text'] = \" \" + r4['metadata']['text']\n",
        "        else:\n",
        "            final_results[file_name]['score'].append(r4['score'])\n",
        "            final_results[file_name]['text'] += r4['metadata']['text']\n",
        "\n",
        "  return final_results\n"
      ],
      "metadata": {
        "id": "xISJZ9YycEix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_result= await filtering_skill_edu_exp(job_data,index)"
      ],
      "metadata": {
        "id": "EdZIk183cS6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Ypc5mq0yq4",
        "outputId": "c7273fc2-93ee-4c40-e8e3-1ef7c2689d36",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/CV_folder/SM Ghuffran 2022.pdf': {'score': [0.538154781],\n",
              "  'reason': 'The skills matched with the skills of the candidate',\n",
              "  'text': '• Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips'},\n",
              " '/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'score': [0.528632641,\n",
              "   0.526331604],\n",
              "  'reason': 'The skills matched with the skills of the candidate',\n",
              "  'text': 'CI/CD  REST API  Docker  MS Office  \\nAWS Services  Problem Solver SKILLS:\\nPython SQL and NoSQL Linux GitHub\\nData Structure and Algorithm Data Analysis\\nCI/CD REST API Docker MS Office\\nAWS Services Problem Solver'}}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scoring_dict = transform_and_sort_response(pf_result)\n",
        "scoring_dict"
      ],
      "metadata": {
        "id": "rtgXdoxqecLX",
        "outputId": "b6090a4d-c78c-45d8-fb1c-56a7c635d784",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/CV_folder/SM Ghuffran 2022.pdf': {'text': '• Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips',\n",
              "  'score': 0.538154781},\n",
              " '/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'text': 'CI/CD  REST API  Docker  MS Office  \\nAWS Services  Problem Solver SKILLS:\\nPython SQL and NoSQL Linux GitHub\\nData Structure and Algorithm Data Analysis\\nCI/CD REST API Docker MS Office\\nAWS Services Problem Solver',\n",
              "  'score': 0.528632641}}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering against remaining fields"
      ],
      "metadata": {
        "id": "-qXDbhBpa3Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter list\n",
        "filter_list = list(scoring_dict.keys())"
      ],
      "metadata": {
        "id": "gt1v-YbMizKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing job_title\n",
        "if \"job_title\" in job_data and (job_data[\"job_title\"] != \"\" and job_data[\"job_title\"] != None):\n",
        "  query = job_data[\"job_title\"]\n",
        "  title_matched = await filtering_other_field(query, filter_list, max_score=0.3)\n",
        "  print (title_matched)\n",
        "else:\n",
        "  title_matched = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nev5g-slvFzw",
        "outputId": "db8d7fd0-24e4-4290-aee6-3815242d3cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'text': \"CI/CD  REST API  Docker  MS Office  \\nAWS Services  Problem Solver SKILLS:\\nPython SQL and NoSQL Linux GitHub\\nData Structure and Algorithm Data Analysis\\nCI/CD REST API Docker MS Office\\nAWS Services Problem Solver H.M. Saqlain Mushtaque  \\nDevOps Trainee  \\nmushtaqesaqlain@gmail.com  \\n+92 301 2335058  \\nKarachi, Pakistan  \\nlinkedin.com/in/saqlain-mushtaque-065058b9/  \\ngithub.com/SAQLAIN-MUSHTAQUE \\n\\nWORK EXPERIENCE:  \\n- Cloud DevOps Trainee  \\nSkipQ, Remote  \\nJan, 2023 – May, 2023  \\nAchievement/Tasks  \\n- Used Infrastructure-as-Code (IaC) constructs to build and operate a full stack production grade web application.  \\n- Wrote RESTful Python application on AWS and developed automated CI/CD pipeline and also automated metrics to roll-back a deployment in case of service degradation.  \\n- Designed multiple applications and implemented some AWS services using AWS Lambda, ALB, SNS, SQS, DynamoDB, Code-Pipeline and etc. SKILLS:  \\nPython  SQL and NoSQL  Linux  GitHub  \\nData Structure and Algorithm  Data Analysis INTERESTS:  \\nCloud Computing  Artificial Intelligence  Data Science \\n\\nI have graduated with a Space Science degree and a passion for cloud computing, machine learning, artificial intelligence, and data science, along with experience in Cloud DevOps, teaching, and customer relations, I am determined to land a job at a cutting-edge company that will not only give me the tools to improve my current abilities but also promote the development of new ones. we can collaboratively work towards achieving the organization's objectives while I gain invaluable experience and knowledge. INTERESTS:\\nCloud Computing Artificial Intelligence Data Science\\nMachine Learning Space Science Physics\\nHistory Travelling\\n```\", 'score': 0.484702736}, '/content/CV_folder/SM Ghuffran 2022.pdf': {'text': '• Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips • Developing automated pipelines for continuous model training and deployment. WORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024 \\n\\nSKILLS\\n• Embedded Systems: RTOS, Microcontrollers (ARM, ESP32), IoT protocols (MQTT, CoAP), Device Drivers, Embedded Linux, UART/SPI/I2C  \\n• Programming Languages: C/C++, Python (Intermediate), Assembly Language, Java, MATLAB  \\n• Frameworks & Libraries: Node.js, React, Flask, TensorFlow (Basic), OpenCV.  \\n• Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips   \\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions. Array Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022 \\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions.   \\n\\nWORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024\\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.\\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.\\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.\\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.\\n• Developing automated pipelines for continuous model training and deployment. \\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions. \\n\\nWORK EXPERIENCE\\nBRB Group  \\nML ENGINEER  \\nJanuary 2022 - February 2024  \\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.  \\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.  \\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.  \\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.  \\n• Developing automated pipelines for continuous model training and deployment.   \\n\\nArray Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022\\n• Experience with web frameworks such as Django or Flask.\\n• Collaborate with team members through branches, merges, and pull requests.\\n• Experience with web frameworks such as Django or Flask.\\n• Use of Git for version control, including branches, merges, and pull requests.\\n• Writing unit tests and performing debugging with frameworks like PyTest or unittests.\\n```\\n ```\\nSYED MUHAMMAD GHUFFRAN\\nComputer System Engineer ```\\nSYED MUHAMMAD GHUFFRAN\\nComputer System Engineer \\n\\nArray Systems Limited  \\nJUNIOR PYTHON DEVELOPER  \\nJuly 2021 - January 2022  \\n• Experience with web frameworks such as Django or Flask.  \\n• Collaborate with team members through branches, merges, and pull requests.  \\n• Experience with web frameworks such as Django or Flask.  \\n• Use of Git for version control, including branches, merges, and pull requests.  \\n• Writing unit tests and performing debugging with frameworks like PyTest or unittest.  \\n```  \\n \\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness. \\n• Embedded Systems: RTOS, Microcontrollers (ARM, ESP32), IoT protocols (MQTT, CoAP), Device Drivers, Embedded Linux, UART/SPI/I2C • Frameworks & Libraries: Node.js, React, Flask, TensorFlow (Basic), OpenCV. 🌐 github.com/SyedGhuffran\\n🔗 linkedin.com/in/syed-ghuffran-272682169 • Use of Git for version control, including branches, merges, and pull requests. \\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.', 'score': 0.453472883}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing job_description\n",
        "if \"job_description\" in job_data and (job_data[\"job_description\"] != \"\" and job_data[\"job_description\"] != None):\n",
        "  query = job_data[\"job_description\"]\n",
        "  description_matched = await filtering_other_field(query, filter_list, max_score=0.3)\n",
        "  print (description_matched)\n",
        "else:\n",
        "  description_matched = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItQ-IP28u9wP",
        "outputId": "da328b93-2c3f-4d09-c7e0-926123d53d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'text': \"H.M. Saqlain Mushtaque  \\nDevOps Trainee  \\nmushtaqesaqlain@gmail.com  \\n+92 301 2335058  \\nKarachi, Pakistan  \\nlinkedin.com/in/saqlain-mushtaque-065058b9/  \\ngithub.com/SAQLAIN-MUSHTAQUE \\n\\nWORK EXPERIENCE:  \\n- Cloud DevOps Trainee  \\nSkipQ, Remote  \\nJan, 2023 – May, 2023  \\nAchievement/Tasks  \\n- Used Infrastructure-as-Code (IaC) constructs to build and operate a full stack production grade web application.  \\n- Wrote RESTful Python application on AWS and developed automated CI/CD pipeline and also automated metrics to roll-back a deployment in case of service degradation.  \\n- Designed multiple applications and implemented some AWS services using AWS Lambda, ALB, SNS, SQS, DynamoDB, Code-Pipeline and etc. CI/CD  REST API  Docker  MS Office  \\nAWS Services  Problem Solver SKILLS:\\nPython SQL and NoSQL Linux GitHub\\nData Structure and Algorithm Data Analysis\\nCI/CD REST API Docker MS Office\\nAWS Services Problem Solver \\n\\n- Math and Computer Teacher  \\nPrime Public School, Karachi  \\nOct, 2021 – Dec, 2022  \\nAchievement/Tasks  \\n- Developing students’ interest in mathematics by using daily life examples.  \\n- Create instruction material that prepares students for careers in Information Technology, including personal computer hardware and software, operating systems, networking, and programming.  \\n- Performing other administrative tasks around the school.\\n```\\nBusiness Development Executive\\nReems Digital\\nJun, 2020 – Oct, 2020\\nAchievement/Tasks\\n– Identify, and secure business opportunities on freelance platforms like Upwork, Freelancer, and fiverr.\\n– Negotiate contract terms with clients and communicate with developers.\\n– Promote the company's services by anticipating or answering the goals of the customers. \\n\\nI have graduated with a Space Science degree and a passion for cloud computing, machine learning, artificial intelligence, and data science, along with experience in Cloud DevOps, teaching, and customer relations, I am determined to land a job at a cutting-edge company that will not only give me the tools to improve my current abilities but also promote the development of new ones. we can collaboratively work towards achieving the organization's objectives while I gain invaluable experience and knowledge.\", 'score': 0.384005308}, '/content/CV_folder/SM Ghuffran 2022.pdf': {'text': '• Developing automated pipelines for continuous model training and deployment. • Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips WORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024 \\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions. \\n\\nWORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024\\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.\\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.\\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.\\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.\\n• Developing automated pipelines for continuous model training and deployment. \\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions.   \\n\\nWORK EXPERIENCE\\nBRB Group  \\nML ENGINEER  \\nJanuary 2022 - February 2024  \\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.  \\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.  \\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.  \\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.  \\n• Developing automated pipelines for continuous model training and deployment.   \\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions. \\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness. Array Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022 \\n\\nArray Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022\\n• Experience with web frameworks such as Django or Flask.\\n• Collaborate with team members through branches, merges, and pull requests.\\n• Experience with web frameworks such as Django or Flask.\\n• Use of Git for version control, including branches, merges, and pull requests.\\n• Writing unit tests and performing debugging with frameworks like PyTest or unittests.\\n```\\n', 'score': 0.356151551}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing job_reponsibilities_optional\n",
        "if \"job_responsibilities_optional\" in job_data and (job_data[\"job_responsibilities_optional\"] != \"\" and job_data[\"job_responsibilities_optional\"] != None):\n",
        "  query = job_data[\"job_responsibilities_optional\"]\n",
        "  optional_responsibilities_matched = await filtering_other_field(query, filter_list, max_score=0.3)\n",
        "  print (optional_responsibilities_matched)\n",
        "else:\n",
        "  optional_responsibilities_matched = {}"
      ],
      "metadata": {
        "id": "IXTL12Tyurrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing required_degree\n",
        "if \"required_degree\" in job_data and (job_data[\"required_degree\"] != \"\" and job_data[\"required_degree\"] != None):\n",
        "  query = job_data[\"required_degree\"]\n",
        "  degree_matched = await filtering_other_field(query, filter_list, max_score=0.3)\n",
        "  print (degree_matched)\n",
        "else:\n",
        "  degree_matched = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nAb1ClLrJfN",
        "outputId": "af69cb34-05aa-4237-ae52-9f679a6ffdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'text': \"SKILLS:\\nPython SQL and NoSQL Linux GitHub\\nData Structure and Algorithm Data Analysis\\nCI/CD REST API Docker MS Office\\nAWS Services Problem Solver CI/CD  REST API  Docker  MS Office  \\nAWS Services  Problem Solver \\n\\nWORK EXPERIENCE:  \\n- Cloud DevOps Trainee  \\nSkipQ, Remote  \\nJan, 2023 – May, 2023  \\nAchievement/Tasks  \\n- Used Infrastructure-as-Code (IaC) constructs to build and operate a full stack production grade web application.  \\n- Wrote RESTful Python application on AWS and developed automated CI/CD pipeline and also automated metrics to roll-back a deployment in case of service degradation.  \\n- Designed multiple applications and implemented some AWS services using AWS Lambda, ALB, SNS, SQS, DynamoDB, Code-Pipeline and etc. \\n\\nI have graduated with a Space Science degree and a passion for cloud computing, machine learning, artificial intelligence, and data science, along with experience in Cloud DevOps, teaching, and customer relations, I am determined to land a job at a cutting-edge company that will not only give me the tools to improve my current abilities but also promote the development of new ones. we can collaboratively work towards achieving the organization's objectives while I gain invaluable experience and knowledge. \\n\\nCERTIFICATES:\\n• Hands-on Introduction to Linux Commands and Shell Scripting\\n• Python for Data Science, AI & Development\\n• Data Science Methodology\\n• Tools for Data Science\\n• What is Data Science?\\n• Integrating Python, SQL, and Tableau\\n• SQL and Relational Databases 101 INTERESTS:  \\nCloud Computing  Artificial Intelligence  Data Science \\n\\n- B.Sc. (Hons.) in GIS & Remote Sensing  \\nInstitute of Space Science and Technology,  \\nKarachi, Pakistan  \\nFeb,2018 – Mar,2022  \\nCourses  \\n- C++ Computer Programming  \\n- Python Programming  \\n- Numerical Computing  \\n- MATLAB Programming  \\n- Digital Signal Processing  \\n- Computational Geographic Information Science  \\n- Statistical Analysis  \\n- Database System  \\n- FYP: Crime Location Analysis Detection. SKILLS:  \\nPython  SQL and NoSQL  Linux  GitHub  \\nData Structure and Algorithm  Data Analysis \\n\\nEDUCATION:  \\n- M.Sc. in GIS & Remote Sensing  \\nInstitute of Space Science and Technology,  \\nKarachi, Pakistan  \\nFeb,2020 – Mar, 2021  \\nCourses  \\n- Data Mining  \\n- Advance GIS and Spatial Analysis  \\n- Satellite Communication  \\n- Spatial Decision Support System  \\n- Advance Remote Sensing  \\n- FYP: Prediction of Groundwater Potential using ArcGIS map. CERTIFICATES:  \\n• Hands-on Introduction to Linux Commands and Shell Scripting INTERESTS:\\nCloud Computing Artificial Intelligence Data Science\\nMachine Learning Space Science Physics\\nHistory Travelling\\n``` H.M. Saqlain Mushtaque  \\nDevOps Trainee  \\nmushtaqesaqlain@gmail.com  \\n+92 301 2335058  \\nKarachi, Pakistan  \\nlinkedin.com/in/saqlain-mushtaque-065058b9/  \\ngithub.com/SAQLAIN-MUSHTAQUE \\n\\n- Math and Computer Teacher  \\nPrime Public School, Karachi  \\nOct, 2021 – Dec, 2022  \\nAchievement/Tasks  \\n- Developing students’ interest in mathematics by using daily life examples.  \\n- Create instruction material that prepares students for careers in Information Technology, including personal computer hardware and software, operating systems, networking, and programming.  \\n- Performing other administrative tasks around the school.\\n```\\nBusiness Development Executive\\nReems Digital\\nJun, 2020 – Oct, 2020\\nAchievement/Tasks\\n– Identify, and secure business opportunities on freelance platforms like Upwork, Freelancer, and fiverr.\\n– Negotiate contract terms with clients and communicate with developers.\\n– Promote the company's services by anticipating or answering the goals of the customers. Math Teacher  \\nMetropolitan Academy  \\nAug, 2018 – Aug, 2021  \\nAchievement/Tasks \\n\\n- Assistant Consultant and Back Office Worker  \\nPowerprep International, Karachi  \\nMay, 2023 – Present  \\nAchievement/Tasks  \\n- Provide information, end to end service while organizing documents for visa and creating the itinerary.  \\n- Identify the websites and online resources that are relevant for data collection using python programming and exploring data using MS-Excel. Machine Learning  Space Science  Physics  \\nHistory  Travelling LANGUAGES:\\nUrdu\\nNative or Bilingual Proficiency\\nEnglish\\nLimited Working Proficiency\", 'score': 0.504132}, '/content/CV_folder/SM Ghuffran 2022.pdf': {'text': '\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions. \\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions. \\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions.   \\n\\nSKILLS\\n• Embedded Systems: RTOS, Microcontrollers (ARM, ESP32), IoT protocols (MQTT, CoAP), Device Drivers, Embedded Linux, UART/SPI/I2C  \\n• Programming Languages: C/C++, Python (Intermediate), Assembly Language, Java, MATLAB  \\n• Frameworks & Libraries: Node.js, React, Flask, TensorFlow (Basic), OpenCV.  \\n• Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips   WORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024 EDUCATION\\nMohammad Ali Jinnah University\\nBE Computer System Engineering\\n2019 - 2023 EDUCATION\\nMohammad Ali Jinnah University  \\nBE Computer System Engineering  \\n2019 - 2023 \\n\\nWORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024\\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.\\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.\\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.\\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.\\n• Developing automated pipelines for continuous model training and deployment. \\n\\nWORK EXPERIENCE\\nBRB Group  \\nML ENGINEER  \\nJanuary 2022 - February 2024  \\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.  \\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.  \\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.  \\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.  \\n• Developing automated pipelines for continuous model training and deployment.   Array Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022 \\n\\nArray Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022\\n• Experience with web frameworks such as Django or Flask.\\n• Collaborate with team members through branches, merges, and pull requests.\\n• Experience with web frameworks such as Django or Flask.\\n• Use of Git for version control, including branches, merges, and pull requests.\\n• Writing unit tests and performing debugging with frameworks like PyTest or unittests.\\n```\\n • Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips \\n\\nArray Systems Limited  \\nJUNIOR PYTHON DEVELOPER  \\nJuly 2021 - January 2022  \\n• Experience with web frameworks such as Django or Flask.  \\n• Collaborate with team members through branches, merges, and pull requests.  \\n• Experience with web frameworks such as Django or Flask.  \\n• Use of Git for version control, including branches, merges, and pull requests.  \\n• Writing unit tests and performing debugging with frameworks like PyTest or unittest.  \\n```  \\n • Frameworks & Libraries: Node.js, React, Flask, TensorFlow (Basic), OpenCV. ```\\nSYED MUHAMMAD GHUFFRAN\\nComputer System Engineer ```\\nSYED MUHAMMAD GHUFFRAN\\nComputer System Engineer \\n• Embedded Systems: RTOS, Microcontrollers (ARM, ESP32), IoT protocols (MQTT, CoAP), Device Drivers, Embedded Linux, UART/SPI/I2C • Programming Languages: C/C++, Python (Intermediate), Assembly Language, Java, MATLAB \\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.', 'score': 0.421959907}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing location\n",
        "if \"required_location\" in job_data and (job_data[\"required_location\"] != \"\" and job_data[\"required_location\"] != None):\n",
        "  query = job_data[\"required_location\"]\n",
        "  location_matched = await filtering_other_field(query, filter_list, max_score=0.3)\n",
        "  print (location_matched)\n",
        "else:\n",
        "  location_matched = {}"
      ],
      "metadata": {
        "id": "JMGd61rVqVv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing Extra info\n",
        "if \"extra_info\" in job_data and (job_data[\"extra_info\"] != \"\" and job_data[\"extra_info\"] != None):\n",
        "  query = job_data[\"extra_info\"]\n",
        "  extra_info_matched = await filtering_other_field(query, filter_list, max_score=0.3)\n",
        "  print (extra_info_matched)\n",
        "else:\n",
        "  extra_info_matched = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ohN-ipfh50e",
        "outputId": "a513e772-2a99-4b34-a8f2-b145cdd10b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/content/CV_folder/SM Ghuffran 2022.pdf': {'text': 'WORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024', 'score': 0.314922392}, '/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'text': '\\n\\n- Assistant Consultant and Back Office Worker  \\nPowerprep International, Karachi  \\nMay, 2023 – Present  \\nAchievement/Tasks  \\n- Provide information, end to end service while organizing documents for visa and creating the itinerary.  \\n- Identify the websites and online resources that are relevant for data collection using python programming and exploring data using MS-Excel.', 'score': 0.30523175}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging with scoring dictionary"
      ],
      "metadata": {
        "id": "bsPgzt0GvWAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_info_matched\n",
        "location_matched\n",
        "degree_matched\n",
        "optional_responsibilities_matched\n",
        "description_matched\n",
        "title_matched\n",
        "\n",
        "# Merge title_matched and description_matched into scoring_dict\n",
        "merged_scoring_dict = merge_dictionaries(scoring_dict,\n",
        "                                         title_matched,\n",
        "                                         description_matched,\n",
        "                                         optional_responsibilities_matched,\n",
        "                                         degree_matched,\n",
        "                                         location_matched,\n",
        "                                         extra_info_matched\n",
        "                                         )\n"
      ],
      "metadata": {
        "id": "T5tj6INNvceP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b130121-6d7c-4370-bd64-02d3e8db8898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before increment: 2.084661514\n",
            "After increment: 4.169323028\n",
            "Before increment: 2.2067044350000002\n",
            "After increment: 4.4134088700000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in merged_scoring_dict.items():\n",
        "  print(f\"key: {key}, score: {value['score']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i9Kj8BOyGGa",
        "outputId": "f476c45d-58fa-4f7c-b750-6b75c4fb44a8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key: /content/CV_folder/SM Ghuffran 2022.pdf, score: 4.169323028\n",
            "key: /content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf, score: 4.4134088700000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Cv Text into merged dict\n",
        "merged_scoring_dict = adding_cv_text(merged_scoring_dict, all_raw_text)\n",
        "merged_scoring_dict"
      ],
      "metadata": {
        "id": "s3KOEgZewoWs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "collapsed": true,
        "outputId": "e863494f-072e-4391-e0ae-c0c816541e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_raw_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-fdfacfca612d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Adding Cv Text into merged dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmerged_scoring_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madding_cv_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_scoring_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_raw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmerged_scoring_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_raw_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation by LLM**"
      ],
      "metadata": {
        "id": "RUaJZdn5JLVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation system Prompt"
      ],
      "metadata": {
        "id": "v8EZRGfG8gJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_prompt = \"\"\"\n",
        "You are an excellent CV evaluator. You have the job_post and are provided with CV text.\n",
        "Your job is to evaluate the CV based on the post and return a matching score (0-5) along with the some information from the CV.\n",
        "\n",
        "**Important:** The score range is between 0 to 5, with specific guidelines below.\n",
        "\n",
        "job_post = {job_post}\n",
        "\n",
        "Score Assessment Guidelines:\n",
        "1. **Score 5: Overqualified**\n",
        "    - **Experience**: Exceeds required years of experience by a significant margin.\n",
        "    - **Skills**: Possesses all required skills and more; may include expertise in advanced or related skills beyond job requirements.\n",
        "    - **Certifications and Courses**: Holds specialized or advanced certifications in the field, often beyond typical qualifications.\n",
        "    - **Education**: Degree aligns closely with the field and is advanced or specialized for the role.\n",
        "    - **Other Aspects**: Demonstrates notable industry achievements or contributions that indicate leadership, innovation, or high recognition.\n",
        "\n",
        "2. **Score 4: Highly Qualified**\n",
        "    - **Experience**: Meets or slightly exceeds experience requirements.\n",
        "    - **Skills**: Covers all required skills well, possibly with some additional relevant skills.\n",
        "    - **Certifications and Courses**: Holds relevant certifications or recent training specific to key skills.\n",
        "    - **Education**: Degree aligns well with the role and may be at a higher level or specialized.\n",
        "    - **Other Aspects**: Displays proactive industry engagement, relevant publications, or a record of strong performance indicators.\n",
        "\n",
        "3. **Score 3: Qualified**\n",
        "    - **Experience**: Meets the minimum experience requirement but doesn’t exceed significantly.\n",
        "    - **Skills**: Has the primary skills required for the job, though without strong breadth beyond essentials.\n",
        "    - **Certifications and Courses**: May have some relevant courses or certifications, though not extensive or advanced.\n",
        "    - **Education**: Degree aligns reasonably well with job requirements.\n",
        "    - **Other Aspects**: Demonstrates a consistent record of performance, stability, and role relevance.\n",
        "\n",
        "4. **Score 2: Partially Qualified**\n",
        "    - **Experience**: Slightly under the required years but relevant, showing potential for growth.\n",
        "    - **Skills**: Has some of the core skills but lacks proficiency in a few key areas.\n",
        "    - **Certifications and Courses**: Lacks certifications specific to the role; few relevant courses.\n",
        "    - **Education**: Degree is related but general, not specialized.\n",
        "    - **Other Aspects**: Shows interest or passion in the industry but lacks strong supporting achievements.\n",
        "\n",
        "5. **Score 1: Minimally Qualified**\n",
        "    - **Experience**: Considerably under the experience requirement.\n",
        "    - **Skills**: Only partially matches the required skills; major gaps are apparent.\n",
        "    - **Certifications and Courses**: No relevant certifications or courses.\n",
        "    - **Education**: Degree is unrelated or significantly different from job requirements.\n",
        "    - **Other Aspects**: Lacks industry engagement or notable achievements; minimal evidence of alignment with the role.\n",
        "\n",
        "6. **Score 0: Not Qualified**\n",
        "    - **Experience**: No relevant experience in the field.\n",
        "    - **Skills**: Lacks any of the core skills necessary for the job.\n",
        "    - **Certifications and Courses**: No certifications or courses relevant to the field.\n",
        "    - **Education**: Educational background unrelated to the field.\n",
        "    - **Other Aspects**: No signs of relevant industry interest, engagement, or potential for growth in the role.\n",
        "\n",
        "**IMPORTANT:** Provide the reason for your rating. The response of this field should be in string.\n",
        "\n",
        "Some Information from CV:\n",
        "1. full_name: Full name of the candidate.\n",
        "2. contact_number: List the contact number of the candidate, if it is available otherwise write \"not_mentioned\".\n",
        "3. emial: Email of the candidate, if it is available otherwise write \"not_mentioned\".\n",
        "4. address: Address of the candidate, if it is available otherwise write \"not_mentioned\".\n",
        "5. highest_degree: Highest degree of the candidate. If there is no such degree then return None.\n",
        "6. bachelors_degree: Bachelors degree or undergraduate degree or associate degree with the number of years, if available. If there is no such degree then return None.\n",
        "7. masters_degree: Masters degree or postgraduate degree with the number of years, if available. If there is no such degree then return None.\n",
        "8. certifications: All the certifications of the candidate, if available. Return the cetificate name, organization (if available), like: [<certificate1-organization>, <certificate2-organization>, <certificate3>, ...]. If there is no such degree then return None.\n",
        "9. total_years_of_experience: The list of field with the total years of experience of the candidate. If there is no experience then return None. The response of this information should comma separated in the string like: \"<field1>:<duration of experience, <field2>:<duration of experience>, ...\".\n",
        "10. skills: A section highlighting both hard skills (technical abilities) and soft skills (interpersonal abilities) that candidate mentioned. If there is no skills then return None.\n",
        "11. linkedin_links: The linkedin profile link of the candidate, if available. If there is no linkedin link then return None.\n",
        "12. achievements_and_awards: A list of achievements and awards earned by the candidate. If there is no achievements and awards then return None.\n",
        "13. projects: A list of the personal or professional projects that the candidate made. If there is no projects then return None.\n",
        "14. publications: A list of the articles or papers that candidate have authored. If there is no publications then return None .\n",
        "15. gitHub_link: The github link of the candidate, if available. If there is no github link then return None.\n",
        "16. Website_link:The portfolio website link of the candidate, if available. If there is no website link then return None.\n",
        "17. languages: A list of languages that candidate speak, along with your proficiency levels. The value this field should be the list dictionary, where every dictionary has the language as the key and the proficiency level as the value. If there is no languages then return  \"not_mentioned\".\n",
        "\n",
        "**IMPORTANT:** All links should be written with the prefix 'https://'.\n",
        "\n",
        "**Output format:**\n",
        "Return your response in JSON format using the structure below:\n",
        "\n",
        "{{  \"score\": int(),\n",
        "    \"reason\": \"\",\n",
        "    \"full_name\": \"\" || \"not_mentioned\",\n",
        "    \"contact_number\": [] || \"not_mentioned\",\n",
        "    \"emial\": \"\" || \"not_mentioned\",\n",
        "    \"address\": \"\" || \"not_mentioned\",\n",
        "    \"highest_degree\": \"\" || None,\n",
        "    \"bachelors_degree\": \"\" || None,\n",
        "    \"masters_degree\": \"\" || None,\n",
        "    \"certifications\": [] || None,\n",
        "    \"total_years_of_experience\": \"\" || None,\n",
        "    \"skills\": [] || None,\n",
        "    \"linkedin_links\": \"\" || None,\n",
        "    \"achievements_and_awards\": [] || None,\n",
        "    \"projects\": [] || None,\n",
        "    \"publications\": [] || None,\n",
        "    \"gitHub_link\": \"\" || None,\n",
        "    \"Website_link\": \"\" || None,\n",
        "    \"languages\": [{{}}] || \"not_mentioned\",\n",
        "}}\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ofSr29cI8jjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass to the LLM (OpenAI)"
      ],
      "metadata": {
        "id": "37N8gONN8kMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_cv(client, evaluate_prompt, job_post, resume_text, llm_model = \"gpt-4o-2024-08-06\"):\n",
        "\n",
        "\n",
        "   # User message\n",
        "    user_message = f\"\"\"Evaluate and extract the information from the resume text: {resume_text}\"\"\"\n",
        "\n",
        "    # Generate the completion\n",
        "    completion = await client.chat.completions.create(\n",
        "        model=llm_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": evaluate_prompt.format(job_post = job_post)},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "    )\n",
        "\n",
        "    # Get the response message\n",
        "    json_message = completion.choices[0].message.content\n",
        "    message = json.loads(json_message)\n",
        "\n",
        "    return message"
      ],
      "metadata": {
        "id": "XrcknSJ78jw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def llm_evaluation(client, merged_scoring_dict, job_post, evaluate_prompt, llm_model=\"gpt-4o-2024-08-06\"):\n",
        "    final_dictionary = {}\n",
        "\n",
        "    # Creating a list of tasks to run concurrently\n",
        "    tasks = [\n",
        "        evaluate_cv(client, evaluate_prompt, job_post, value['CV_text'])\n",
        "        for file_name, value in merged_scoring_dict.items()\n",
        "    ]\n",
        "\n",
        "    # Await all tasks concurrently\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Populating the final dictionary with responses\n",
        "    for (file_name, value), response in zip(merged_scoring_dict.items(), responses):\n",
        "        response['score'] += value['score']  # Adding the previous score\n",
        "        response['score'] = round((response['score']/10)*100, 2)\n",
        "        final_dictionary[file_name] = response\n",
        "        final_dictionary[file_name]['CV_text'] = value['CV_text']\n",
        "\n",
        "    # Sorting the dictionary by score in descending order\n",
        "    sorted_final_dictionary = dict(\n",
        "        sorted(final_dictionary.items(), key=lambda item: item[1]['score'], reverse=True)\n",
        "    )\n",
        "\n",
        "    return sorted_final_dictionary"
      ],
      "metadata": {
        "id": "dxOZJdhOG_Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passing with LLM To Evaluate**"
      ],
      "metadata": {
        "id": "yl2pC2QGx2mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = await llm_evaluation(client, merged_scoring_dict, query, evaluate_prompt)\n",
        "final_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lm5X1OwnK3Tw",
        "outputId": "c7be1264-08a9-45f9-a0b1-a43d27c15f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/CV_folder/SM Ghuffran 2022.pdf': {'score': 67.89,\n",
              "  'reason': \"The candidate is highly qualified with relevant experience and skills in line with the job post's requirements. They have over two years of experience in fields related to machine learning engineering and web development with strong skills in programming and development tools. However, there is no mention of multicultural or international exposure, certifications, or advanced degrees that would suggest overqualification.\",\n",
              "  'full_name': 'SYED MUHAMMAD GHUFFRAN',\n",
              "  'contact_number': ['+92347-2360709'],\n",
              "  'emial': 'ghuffi016@gmail.com',\n",
              "  'address': 'Gulistan e Johar Block 7 Karachi',\n",
              "  'highest_degree': 'BE Computer System Engineering',\n",
              "  'bachelors_degree': 'BE Computer System Engineering:4',\n",
              "  'masters_degree': None,\n",
              "  'certifications': None,\n",
              "  'total_years_of_experience': 'ML ENGINEER:2, JUNIOR PYTHON DEVELOPER:0.5',\n",
              "  'skills': ['Embedded Systems',\n",
              "   'RTOS',\n",
              "   'Microcontrollers (ARM, ESP32)',\n",
              "   'IoT protocols (MQTT, CoAP)',\n",
              "   'Device Drivers',\n",
              "   'Embedded Linux',\n",
              "   'UART/SPI/I2C',\n",
              "   'C/C++',\n",
              "   'Python',\n",
              "   'Assembly Language',\n",
              "   'Java',\n",
              "   'MATLAB',\n",
              "   'Node.js',\n",
              "   'React',\n",
              "   'Flask',\n",
              "   'TensorFlow',\n",
              "   'OpenCV',\n",
              "   'Git',\n",
              "   'Docker',\n",
              "   'Jenkins',\n",
              "   'JIRA',\n",
              "   'Bitbucket',\n",
              "   'Visual Studio',\n",
              "   'Eclipse'],\n",
              "  'linkedin_links': 'https://linkedin.com/in/syed-ghuffran-272682169',\n",
              "  'achievements_and_awards': None,\n",
              "  'projects': None,\n",
              "  'publications': None,\n",
              "  'gitHub_link': 'https://github.com/SyedGhuffran',\n",
              "  'Website_link': None,\n",
              "  'languages': 'not_mentioned',\n",
              "  'CV_text': '```\\nSYED MUHAMMAD GHUFFRAN\\nComputer System Engineer\\n\\nCONTACT\\n📞 +92347-2360709\\n✉️ ghuffi016@gmail.com\\n🏠 Gulistan e Johar Block 7 Karachi\\n🌐 github.com/SyedGhuffran\\n🔗 linkedin.com/in/syed-ghuffran-272682169\\n\\nEDUCATION\\nMohammad Ali Jinnah University\\nBE Computer System Engineering\\n2019 - 2023\\n\\nSKILLS\\n• Embedded Systems: RTOS, Microcontrollers (ARM, ESP32), IoT protocols (MQTT, CoAP), Device Drivers, Embedded Linux, UART/SPI/I2C\\n• Programming Languages: C/C++, Python (Intermediate), Assembly Language, Java, MATLAB\\n• Frameworks & Libraries: Node.js, React, Flask, TensorFlow (Basic), OpenCV.\\n• Development Tools: Git, Docker, Jenkins, JIRA, Bitbucket, Visual Studio, Eclips\\n\\nPROFILE\\nAs a Computer Systems Engineer registered with PEC and specializing in machine learning, I bring extensive technical expertise in both software and hardware engineering. My professional journey is marked by deep involvement in designing and deploying ML solutions using advanced algorithms and data handling techniques. I focus on data preprocessing, feature engineering, and robust ML model training. My technical skills extend to FPGA systems and embedded systems, enhancing my capabilities in creating optimized hardware solutions.\\n\\nWORK EXPERIENCE\\nBRB Group\\nML ENGINEER\\nJanuary 2022 - February 2024\\n• Designing and implementing machine learning algorithms using frameworks such as TensorFlow, PyTorch, and scikit-learn.\\n• Conducting extensive data preprocessing, feature engineering, and data augmentation to ensure high-quality input data for training models.\\n• Optimizing model performance through hyperparameter tuning, cross-validation, and regularization techniques.\\n• Integrating ML models into production environments, leveraging techniques for scalability and robustness.\\n• Developing automated pipelines for continuous model training and deployment.\\n\\nArray Systems Limited\\nJUNIOR PYTHON DEVELOPER\\nJuly 2021 - January 2022\\n• Experience with web frameworks such as Django or Flask.\\n• Collaborate with team members through branches, merges, and pull requests.\\n• Experience with web frameworks such as Django or Flask.\\n• Use of Git for version control, including branches, merges, and pull requests.\\n• Writing unit tests and performing debugging with frameworks like PyTest or unittest.\\n```\\n\\n'},\n",
              " '/content/CV_folder/H.M. Saqlain Mushtaque Resume 2023.pdf': {'score': 66.22,\n",
              "  'reason': 'The candidate has relevant educational background and some work experience in related fields, but the experience duration and depth in DevOps is limited. They possess relevant skills and have undertaken relevant training, but there is no extensive experience or advanced qualifications that demonstrate exceeding competency.',\n",
              "  'full_name': 'H.M. Saqlain Mushtaque',\n",
              "  'contact_number': ['+92 301 2335058'],\n",
              "  'emial': 'mushtaquesaqlain@gmail.com',\n",
              "  'address': 'Karachi, Pakistan',\n",
              "  'highest_degree': 'M.Sc. in GIS & Remote Sensing',\n",
              "  'bachelors_degree': 'B.Sc. (Hons.) in GIS & Remote Sensing',\n",
              "  'masters_degree': 'M.Sc. in GIS & Remote Sensing',\n",
              "  'certifications': ['Hands-on Introduction to Linux Commands and Shell Scripting',\n",
              "   'Python for Data Science, AI & Development',\n",
              "   'Data Science Methodology',\n",
              "   'Tools for Data Science',\n",
              "   'What is Data Science?',\n",
              "   'Integrating Python, SQL, and Tableau',\n",
              "   'SQL and Relational Databases 101'],\n",
              "  'total_years_of_experience': 'Cloud DevOps Trainee:0.4, Assistant Consultant and Back Office Worker:0.25, Math and Computer Teacher:1.2, Business Development Executive:0.33, Math Teacher:3.0',\n",
              "  'skills': ['Python',\n",
              "   'SQL and NoSQL',\n",
              "   'Linux',\n",
              "   'GitHub',\n",
              "   'Data Structure and Algorithm',\n",
              "   'Data Analysis',\n",
              "   'CI/CD',\n",
              "   'REST API',\n",
              "   'Docker',\n",
              "   'MS Office',\n",
              "   'AWS Services',\n",
              "   'Problem Solver'],\n",
              "  'linkedin_links': 'https://linkedin.com/in/saqlain-mushtaque-065058b9',\n",
              "  'achievements_and_awards': None,\n",
              "  'projects': ['Loan Eligibility Prediction', 'Employee Absenteeism Study'],\n",
              "  'publications': None,\n",
              "  'gitHub_link': 'https://github.com/SAQLAIN-MUSHTAQUE',\n",
              "  'Website_link': None,\n",
              "  'languages': [{'Urdu': 'Native or Bilingual Proficiency'},\n",
              "   {'English': 'Limited Working Proficiency'}],\n",
              "  'CV_text': \"H.M. Saqlain Mushtaque  \\nDevOps Trainee  \\nmushtaquesaqlain@gmail.com  \\n+92 301 2335058  \\nKarachi, Pakistan  \\nlinkedin.com/in/saqlain-mushtaque-065058b9/  \\ngithub.com/SAQLAIN-MUSHTAQUE  \\n\\nEDUCATION:  \\n- M.Sc. in GIS & Remote Sensing  \\n  Institute of Space Science and Technology,  \\n  Karachi, Pakistan  \\n  Feb, 2020 – Mar, 2021  \\n  Courses  \\n  - Data Mining  \\n  - Advance GIS and Spatial Analysis  \\n  - Satellite Communication  \\n  - Spatial Decision Support System  \\n  - Advance Remote Sensing  \\n  - FYP: Prediction of Groundwater Potential using ArcGIS map.  \\n\\n- B.Sc. (Hons.) in GIS & Remote Sensing  \\n  Institute of Space Science and Technology,  \\n  Karachi, Pakistan  \\n  Feb, 2018 – Mar, 2022  \\n  Courses  \\n  - C++ Computer Programming  \\n  - Python Programming  \\n  - Numerical Computing  \\n  - MATLAB Programming  \\n  - Digital Signal Processing  \\n  - Computational Geographic Information Science  \\n  - Statistical Analysis  \\n  - Database System  \\n  - FYP: Crime Location Analysis Detection.  \\n\\nWORK EXPERIENCE:  \\n- Cloud DevOps Trainee  \\n  SkipQ, Remote  \\n  Jan, 2023 – May, 2023  \\n  Achievement/Tasks  \\n  - Used Infrastructure-as-Code (IaC) constructs to build and operate a full stack production grade web application.  \\n  - Wrote RESTful Python application on AWS and developed automated CI/CD pipeline and also automated metrics to roll-back a deployment in case of service degradation.  \\n  - Designed multiple applications and implemented some AWS services using AWS Lambda, ALB, SNS, SQS, DynamoDB, Code-Pipeline and etc.  \\n\\n- Assistant Consultant and Back Office Worker  \\n  Powerprep International, Karachi  \\n  May, 2023 – Present  \\n  Achievement/Tasks  \\n  - Provide information, end to end service while organizing documents for visa and creating the itinerary.  \\n  - Identify the websites and online resources that are relevant for data collection using python programming and exploring data using MS-Excel.  \\n\\n- Math and Computer Teacher  \\n  Prime Public School, Karachi  \\n  Oct, 2021 – Dec, 2022  \\n  Achievement/Tasks  \\n  - Developing students’ interest in mathematics by using daily life examples.  \\n  - Create instruction material that prepares students for careers in Information Technology, including personal computer hardware and software, operating systems, networking, and programming.  \\n  - Performing other administrative tasks around the school.\\n```\\nBusiness Development Executive\\nReems Digital                           Jun, 2020 – Oct, 2020\\n  Achievement/Tasks\\n  – Identify, and secure business opportunities on freelance platforms like Upwork, Freelancer, and fiverr.\\n  – Negotiate contract terms with clients and communicate with developers.\\n  – Promote the company's services by anticipating or answering the goals of the customers.\\n\\nMath Teacher\\nMetropolitan Academy                     Aug, 2018 – Aug, 2021\\n  Achievement/Tasks\\n  – Taught mathematical skills and principles to secondary students using real-world, application-based examples to maximize \\n    learning opportunities.\\n  – Assessed submitted class assignments, determined grades and reviewed work with struggling students to boost success \\n    chance.\\n  – Prepared and implemented lesson plans covering required course topics.\\n\\nCERTIFICATES:\\n  • Hands-on Introduction to Linux Commands and Shell Scripting                   Apr,2023 – May,2023\\n  • Python for Data Science, AI & Development                                        Nov, 2022 – Dec, 2022\\n  • Data Science Methodology                                                        Oct, 2022 – Oct, 2022\\n  • Tools for Data Science                                                          Sep, 2022 – Sep, 2022\\n  • What is Data Science?                                                           Jul, 2022 – Aug, 2022\\n  • Integrating Python, SQL, and Tableau\\n  • SQL and Relational Databases 101\\n\\nPERSONAL PROJECT:\\n  Loan Eligibility Prediction\\n  – Pandas, matplotlib, scikit-learn.\\n  – Find out the Eligibility of loan by entering the people’s data.\\n\\n  Employee Absenteeism Study\\n  – Numpy, Pandas, scikit-learn.\\n  – Collecting Employee data to identify the reasons for absenteeism.\\n  – To identify steps required to decrease the rate of absenteeism.\\n\\nSKILLS:\\n  Python            SQL and NoSQL          Linux            GitHub\\n  Data Structure and Algorithm            Data Analysis           CI/CD\\n  REST API        Docker          MS Office       AWS Services            Problem Solver\\n\\nLANGUAGES:\\n  Urdu           Native or Bilingual Proficiency\\n  English        Limited Working Proficiency\\n\\nINTERESTS:\\n  Cloud Computing             Artificial Intelligence        Data Science\\n  Machine Learning            Space Science                  Physics\\n  History                    Travelling\\n```\\n\\n\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNsoJF-6UVyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}